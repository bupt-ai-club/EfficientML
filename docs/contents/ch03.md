# 第3章 Pruning and Sparsity



[Learning both weights and connections for efficient neural network](https://arxiv.org/abs/1506.02626)[code](https://github.com/jack-willturner/deep-compression)

&emsp;&emsp;提出了一种以保持原始精度的方式修剪网络连接的方法。在初始训练阶段之后，我们移除权值低于阈值的所有连接。这种修剪将密集的、完全连接的层转换为稀疏层。第一阶段学习网络的拓扑结构——学习哪些连接是重要的，并去除不重要的连接。然后我们重新训练稀疏网络，这样剩余的连接可以补偿被删除的连接。修剪和再训练阶段可以迭代重复，以进一步降低网络的复杂性。


